{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   # Importing NLP Tool Kit\n",
    "import string   #For getting punctuations from a string\n",
    "from nltk.corpus import stopwords  # For getting stopwords like 'the,in,been...'\n",
    "import re  # For Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('Tweet_Sentiment_Analysis_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "id                                                          \n",
       "1       0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "2       0  Finally a transparant silicon case ^^ Thanks t...\n",
       "3       0  We love this! Would you go? #talk #makememorie...\n",
       "4       0   I'm wired I know I'm George I was made that way \n",
       "5       1  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.head() #Getting the head of the data we have read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 666 entries, 1 to 666\n",
      "Data columns (total 2 columns):\n",
      "label    666 non-null int64\n",
      "tweet    666 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "tweet_data.info() #Getting info of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(tweet):\n",
    "    \"\"\"\n",
    "    Tokenization(Splitting a sentence into list of words) of the tweet\n",
    "      1. Removing URLs from the tweet\n",
    "      2. Removing Punctuations from the tweet\n",
    "      3. Removing stopwords (the,he,she,it)\n",
    "    \"\"\"\n",
    "    no_url_tweet = re.sub('http\\S+',\"\", tweet) #URL Removing\n",
    "    no_url_tweet2 = re.sub('twitter\\S+','', no_url_tweet)\n",
    "    no_punc_tweet = [char for char in no_url_tweet2 if char not in string.punctuation] #Removing Punctuations\n",
    "    #After removing Punctuations,no_punc_tweet is a list of characters...So we make it as words using .join()\n",
    "    #method and store that words list in no_punc_tweet\n",
    "    no_punc_tweet = ''.join(no_punc_tweet)\n",
    "    #Now we are using stopwords module from NLTK package to remove StopWords like (i,me,my,myself,we,our,etc...)\n",
    "    cleaned_tweet = [word for word in no_punc_tweet.split() if word.lower() not in stopwords.words('english')]\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1    [fingerprint, Pregnancy, Test, android, apps, ...\n",
       "2    [Finally, transparant, silicon, case, Thanks, ...\n",
       "3    [love, Would, go, talk, makememories, unplug, ...\n",
       "4             [Im, wired, know, Im, George, made, way]\n",
       "5    [amazing, service, Apple, wont, even, talk, qu...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the text_processor() method on all tweets for checking...\n",
    "tweet_data['tweet'].apply(text_processor).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization - Making a list of words to a vector which Machine Learning Algorithm can understand\n",
    "\n",
    "    #1. Count no. of times a word occured in a tweet(Also Known As 'Term Frequency')\n",
    "    #2. Knowing how important the word is by using IDF(Inver Document Frequency)\n",
    "    #3. Normalize the vectors to unit length, to abstract from the original text length\n",
    "\n",
    "# We are using scikit-learn's CountVectorizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Transform each word into a vector we are initialising an instance for CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=text_processor).fit(tweet_data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPad\n"
     ]
    }
   ],
   "source": [
    "# Checking which word is having the 2220 index\n",
    "print(bow_transformer.get_feature_names()[2220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3816\n"
     ]
    }
   ],
   "source": [
    "# To know that how many words are there in our whole data\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming each word into a vector by using the instance of CountVectorizer\n",
    "tweets_bow = bow_transformer.transform(tweet_data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF  from scikit-learn\n",
    "# To know how important that word is for our analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(tweets_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying TfidfTransformer to all the tweets\n",
    "tweets_tfidf = tfidf_transformer.transform(tweets_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we use Naive bayes Algorithm to classify\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_model = MultinomialNB().fit(tweets_tfidf, tweet_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pos_neg_model.predict(tweets_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train, tweet_test, label_train, label_test = train_test_split(tweet_data['tweet'], tweet_data['label'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_processor at 0x7fe9b0e2e510>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using pipeline method from sklearn.pipeline for simplicity\n",
    "# By using pipeline we can do Vectorization, TfidfTransformation, Classification of data by a model...\n",
    "# All can be done in 1 step without doing all Manually\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_processor)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('Classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(tweet_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Prediction\n",
    "pipeline_pred = pipeline.predict(tweet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       156\n",
      "           1       0.60      0.07      0.12        44\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       200\n",
      "   macro avg       0.69      0.53      0.50       200\n",
      "weighted avg       0.75      0.79      0.71       200\n",
      "\n",
      "Accuracy Score:\n",
      "\t 0.785\n"
     ]
    }
   ],
   "source": [
    "# Knowing the Classification Report to know the Accuracy of our prediction\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(label_test, pipeline_pred))\n",
    "print('Accuracy Score:\\n\\t', accuracy_score(label_test, pipeline_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
